{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aebe0dd",
   "metadata": {},
   "source": [
    "# <center> Vis√£o Computacional ‚Äî Neur√¥nio Artificial</center>\n",
    "\n",
    "---\n",
    "\n",
    "##  O que √© um Neur√¥nio Artificial?\n",
    "\n",
    "Um **neur√¥nio artificial** √© uma unidade computacional que imita o comportamento dos neur√¥nios biol√≥gicos.  \n",
    "Ele recebe entradas, aplica pesos, soma tudo com um vi√©s e passa esse valor por uma fun√ß√£o de ativa√ß√£o.\n",
    "\n",
    "\n",
    "\n",
    "##  F√≥rmula da Soma Ponderada\n",
    "\n",
    "A sa√≠da antes da ativa√ß√£o √© dada por:\n",
    "\n",
    "S = (w * x) + b\n",
    "\n",
    "###  Interpreta√ß√£o dos termos\n",
    "\n",
    "- **\\(S\\)** ‚Äî Soma ponderada (tamb√©m chamada de *input l√≠quido*).  \n",
    "- **\\(w\\)** ‚Äî Peso associado √† entrada.  \n",
    "- **\\(x\\)** ‚Äî Valor da entrada.  \n",
    "- **\\(b\\)** ‚Äî Vi√©s (bias), que desloca a fun√ß√£o de ativa√ß√£o.\n",
    "\n",
    "Se houver v√°rias entradas:\n",
    "\n",
    "S = (w1 * x1) + (w2 * x2) + ... + (wn * xn) + b\n",
    "\n",
    "\n",
    "\n",
    "##  Fun√ß√£o de Ativa√ß√£o\n",
    "\n",
    "Ap√≥s calcular \\(S\\), o valor passa por uma **fun√ß√£o de ativa√ß√£o**, que decide a sa√≠da final do neur√¥nio:\n",
    "\n",
    "### Sigmoid  \n",
    "√â um tipo de neur√¥nio em uma rede neural que usa a fun√ß√£o sigmoid (em forma de \"S\") como sua fun√ß√£o de ativa√ß√£o.  \n",
    "  \n",
    "Sua principal fun√ß√£o √© converter qualquer valor de entrada (um n√∫mero real) em uma sa√≠da suave e cont√≠nua que est√° sempre no intervalo entre 0 e 1.  \n",
    "- Se o valor de entrada for muito negativo, a sa√≠da ser√° pr√≥xima de 0.  \n",
    "- Se o valor de entrada for muito positivo, a sa√≠da ser√° pr√≥xima de 1.  \n",
    "- Se o valor estiver pr√≥ximo de 0, a sa√≠da estar√° pr√≥xima de 0,5.\n",
    "\n",
    "E = 1/2 * (t - y)¬≤\n",
    "\n",
    "Outras fun√ß√µes comuns:\n",
    "\n",
    "- **Tanh** (hiperb√≥lica)\n",
    "- **ReLU**\n",
    "- **Leaky ReLU**\n",
    "- **Exponential Linear Unit (ELU)**\n",
    "\n",
    "\n",
    "\n",
    "##  Fun√ß√£o de Erro (Loss Function)\n",
    "\n",
    "A fun√ß√£o de erro mede o qu√£o distante est√° o resultado desejado:\n",
    "\n",
    "dE/dw = (dE/dy) * (dy/dS) * (dS/dw)\n",
    "\n",
    "Onde:\n",
    "\n",
    "- \\(t\\) = *target* (o valor desejado)  \n",
    "- \\(y\\) = sa√≠da do neur√¥nio ap√≥s a ativa√ß√£o  \n",
    "\n",
    "\n",
    "\n",
    "## Ajuste dos Pesos (Backpropagation)\n",
    "\n",
    "O objetivo √© ajustar o peso **w** para minimizar o erro.\n",
    "\n",
    "Para isso, usamos a **regra da cadeia**, que permite quebrar derivadas complexas em partes menores.\n",
    "\n",
    "Exemplo simples da regra da cadeia:\n",
    "\n",
    "Se:\n",
    "u(x) = (x + 1) * 4\n",
    "\n",
    "Ent√£o:\n",
    "du/dx = 4\n",
    "\n",
    "\n",
    "### Aplicando ao neur√¥nio\n",
    "\n",
    "A derivada do erro em rela√ß√£o ao peso √©:\n",
    "\n",
    "dE/dw = (dE/dy) * (dy/dS) * (dS/dw)\n",
    "\n",
    "Interpreta√ß√£o de cada termo:\n",
    "\n",
    "1. **dE/dy** ‚Äî quanto o erro muda quando a sa√≠da y muda  \n",
    "2. **dy/dS** ‚Äî quanto a fun√ß√£o de ativa√ß√£o muda quando S muda  \n",
    "3. **dS/dw** ‚Äî quanto a soma ponderada S muda quando o peso w muda\n",
    "\n",
    "Como:\n",
    "\n",
    "S = (w * x) + b\n",
    "\n",
    "Ent√£o:\n",
    "\n",
    "dS/dw = x\n",
    "\n",
    "\n",
    "##  Recursos Visuais\n",
    "\n",
    "F√≥rmulas:  \n",
    "\n",
    "üîó https://excalidraw.com/#room=2e2f0832aab9b8c40d36,Qn0tn30UH_lvd19ZgVcO3A\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
