{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aebe0dd",
   "metadata": {},
   "source": [
    "# <center> Vis√£o Computacional ‚Äî Neur√¥nio Artificial</center>\n",
    "\n",
    "---\n",
    "\n",
    "##  O que √© um Neur√¥nio Artificial?\n",
    "\n",
    "Um **neur√¥nio artificial** √© uma unidade computacional que imita o comportamento dos neur√¥nios biol√≥gicos.  \n",
    "Ele recebe entradas, aplica pesos, soma tudo com um vi√©s e passa esse valor por uma fun√ß√£o de ativa√ß√£o.\n",
    "\n",
    "\n",
    "\n",
    "##  F√≥rmula da Soma Ponderada\n",
    "\n",
    "A sa√≠da antes da ativa√ß√£o √© dada por:\n",
    "\n",
    "``S = (w * x) + b``\n",
    "\n",
    "###  Interpreta√ß√£o dos termos\n",
    "\n",
    "- **\\(S\\)** ‚Äî Soma ponderada (tamb√©m chamada de *input l√≠quido*).  \n",
    "- **\\(w\\)** ‚Äî Peso associado √† entrada.  \n",
    "- **\\(x\\)** ‚Äî Valor da entrada.  \n",
    "- **\\(b\\)** ‚Äî Vi√©s (bias), que desloca a fun√ß√£o de ativa√ß√£o.\n",
    "\n",
    "Se houver v√°rias entradas:\n",
    "\n",
    "``S = (w1 * x1) + (w2 * x2) + ... + (wn * xn) + b``\n",
    "\n",
    "\n",
    "\n",
    "#  Fun√ß√£o de Ativa√ß√£o\n",
    "\n",
    "Ap√≥s calcular \\(S\\), o valor passa por uma **fun√ß√£o de ativa√ß√£o**, que decide a sa√≠da final do neur√¥nio. Imagine que cada neur√¥nio da rede neural √© como uma *pessoa tomando uma decis√£o*.\n",
    "\n",
    "1. Ela recebe informa√ß√µes (n√∫meros)\n",
    "2. Faz uma conta (soma + pesos)  \n",
    "3. Decide se passa adiante ou n√£o.\n",
    "\n",
    "Em resumo, √© a decis√£o, d√£o \" *intelig√™ncia* \". Sem ela a rede s√≥ faria contas lineares e n√£o aprenderia coisas complexas como imagens, voz, texto, padr√µes. Al√©m disso, ela decide quanto e quais neur√¥nios deve ativar.\n",
    "\n",
    "A fun√ß√£o de ativa√ß√£o funciona da mesma forma em Dense e Conv2D, o que muda √© o tipo de dado que chega at√© ela.\n",
    "\n",
    "***Camadas internas aprendem padr√µes,  \n",
    "Camada final responde a pergunta***  \n",
    "\n",
    "\n",
    "## ReLU (Rectified Linear Unit)  \n",
    "\n",
    "`f(x) = max(0, x)`\n",
    "\n",
    "Ela transforma cada neur√¥nio em um interruptor inteligente. Se o padr√£o n√£o existe, neur√¥nio fica desligado. Se o padr√£o existe, neur√¥nio passa o sinal inteiro.\n",
    "\n",
    "- Se o valor for negativo, **vira 0**,\n",
    "- Se o valor for positivo, **continua igual**\n",
    "\n",
    "Em resumo, n√£o \"ameniza\", n√£o \"suaviza\", deixa passar cru. Assim, ela cria esparsidade e muitos neur√¥nios ficam em zero. Isso significa que, s√≥ alguns neur√¥nios \"falam\", outros ficam quietos e a rede aprende quais padr√µes importam.\n",
    "\n",
    "Em Dense, cada neur√¥nio s√≥ responde se detectar algo relevante nos dados de entrada, aprendendo combina√ß√µes n√£o-lineares de features e criando limites de decis√£o complexos.  \n",
    "\n",
    "Em Conv2D, o filtro detecta um padr√£o (borda, textura, curva) e a fun√ß√£o remove ativa√ß√µes fracas (negativas) e mant√©m apenas padr√µes fortes.\n",
    "\n",
    "**Quando usar:**\n",
    " - Camadas internas ou ocultas  \n",
    " - Redes profundas  \n",
    " - CNNs (imagens, v√≠deo)\n",
    " Se n√£o sabe qual usar, use ReLU.\n",
    "\n",
    " **Quando n√£o usar**:\n",
    " - Na camada final, a camada de sa√≠da √© de classifica√ß√£o probabil√≠stica  \n",
    " - Quando precisa de sa√≠da negativa  \n",
    " - Em problemas muito sens√≠veis a neur√¥nios mortos\n",
    "\n",
    " Se o neur√¥nio s√≥ recebe valores negativos -> sa√≠da sempre 0 -> gradiente 0 -> nunca aprende.\n",
    "\n",
    "## Sigmoid  \n",
    "  \n",
    "*Qual a probabilidade disso ser verdadeiro?*  \n",
    "\n",
    "√â uma fun√ß√£o que comprime qualquer valor real para o intervalor 0 - 1. Possui forma de \"S\".  \n",
    "  \n",
    "Sua principal fun√ß√£o √© converter qualquer valor de entrada (um n√∫mero real) em uma sa√≠da suave e cont√≠nua que est√° sempre no intervalo entre 0 e 1.  \n",
    "- Se o valor de entrada for muito negativo, a sa√≠da ser√° pr√≥xima de 0.  \n",
    "- Se o valor de entrada for muito positivo, a sa√≠da ser√° pr√≥xima de 1.  \n",
    "- Se o valor estiver pr√≥ximo de 0, a sa√≠da estar√° pr√≥xima de 0,5.  \n",
    "  \n",
    "Essa fun√ß√£o satura facilmente, em entradas grandes possui um gradiente quase zero e assim a rede para de aprender. Por isso, esse √© o grande problema dela em camadas ocultas.  \n",
    "\n",
    "Em Dense, √© usada quase exclusivamente na √∫ltima camada pois funciona com classifica√ß√£o bin√°ria.  \n",
    "Em Conv2D, √© raramente usada por√©m pode aparecer em m√°scaras (segmenta√ß√£o bin√°ria) e attention gates, mas nunca em camadas intermedi√°rias.\n",
    "\n",
    "**Quando usar:**  \n",
    "- Sa√≠da bin√°ria  \n",
    "- Probabilidade de evento  \n",
    "- Logistic Regression Neural  \n",
    "  \n",
    "**Quando n√£o usar:**  \n",
    "- Camadas ocultas  \n",
    "- CNNs intermedi√°rias  \n",
    "- Redes grandes\n",
    "  \n",
    "## Tahn (Tangente Hiperb√≥lica)  \n",
    "√â uma vers√£o centrada no zero da Sigmoid, possui um intervalo de -1 a 1.  \n",
    "\n",
    "√â melhor que a Sigmoid pois os dados ficam mais balanceados, por√©m ainda sofre de *vanishing gradient*. Hoje √© frequentemente substitu√≠da pela ReLu.  \n",
    "\n",
    "**Quando usar:**  \n",
    "- Dados normalizados entre -1 e 1  \n",
    "- Redes pequenas  \n",
    "- Alguns casos em RNNs  \n",
    "\n",
    "**Quando n√£o usar:**  \n",
    "- Redes profundas\n",
    "- CNNs modernas\n",
    "\n",
    "Na d√∫vida, vai de ReLU.\n",
    "\n",
    "## Softmax  \n",
    "Transforma um vetor de n√∫meros em uma distribui√ß√£o de probabilidade  \n",
    "\n",
    "Na pr√°tica os neur√¥nios competem entre si. Quanto maior um, menor os outros. Por isso, √© uma fun√ß√£o usada somente na sa√≠da para classifica√ß√£o de multiclasse. \n",
    "\n",
    "Em Dense, cada sa√≠da √© uma probabilidade de uma classe que soma no total 1.  \n",
    "Em Conv2D, n√£o se usa diretamente:   \n",
    "Conv -> flatten -> Dense -> Softmax  \n",
    "\n",
    "**Quando usar:**  \n",
    "- Classifica√ß√£o multiclasse exclusiva  \n",
    "- Uma √∫nica classe correta  \n",
    "  \n",
    "**Quando n√£o usar:**  \n",
    " - Classifica√ß√£o multilabel  \n",
    " - Problemas de regress√£o.\n",
    "\n",
    " ## Linear  \n",
    "\n",
    " A linear n√£o altera o valor. O neur√¥nio apenas entrega a soma ponderada, por isso √© usada exclusivamente na sa√≠da de regress√£o.  \n",
    " N√£o √© comumente usada em Conv2D.\n",
    "\n",
    " **Quando usar:**  \n",
    "- Regress√£o (temperatura, pre√ßo, idade)\n",
    "- Valores cont√≠nuos\n",
    "\n",
    " **Quando n√£o usar:**  \n",
    "- Camadas ocultas\n",
    "\n",
    "##  Fun√ß√£o de Erro (Loss Function)\n",
    "\n",
    "A fun√ß√£o de erro mede o qu√£o distante est√° o resultado desejado:\n",
    "\n",
    "dE/dw = (dE/dy) * (dy/dS) * (dS/dw)\n",
    "\n",
    "Onde:\n",
    "\n",
    "- \\(t\\) = *target* (o valor desejado)  \n",
    "- \\(y\\) = sa√≠da do neur√¥nio ap√≥s a ativa√ß√£o  \n",
    "\n",
    "\n",
    "\n",
    "## Ajuste dos Pesos (Backpropagation)\n",
    "\n",
    "O objetivo √© ajustar o peso **w** para minimizar o erro.\n",
    "\n",
    "Para isso, usamos a **regra da cadeia**, que permite quebrar derivadas complexas em partes menores.\n",
    "\n",
    "Exemplo simples da regra da cadeia:\n",
    "\n",
    "Se:\n",
    "u(x) = (x + 1) * 4\n",
    "\n",
    "Ent√£o:\n",
    "du/dx = 4\n",
    "\n",
    "\n",
    "### Aplicando ao neur√¥nio\n",
    "\n",
    "A derivada do erro em rela√ß√£o ao peso √©:\n",
    "\n",
    "dE/dw = (dE/dy) * (dy/dS) * (dS/dw)\n",
    "\n",
    "Interpreta√ß√£o de cada termo:\n",
    "\n",
    "1. **dE/dy** ‚Äî quanto o erro muda quando a sa√≠da y muda  \n",
    "2. **dy/dS** ‚Äî quanto a fun√ß√£o de ativa√ß√£o muda quando S muda  \n",
    "3. **dS/dw** ‚Äî quanto a soma ponderada S muda quando o peso w muda\n",
    "\n",
    "Como:\n",
    "\n",
    "S = (w * x) + b\n",
    "\n",
    "Ent√£o:\n",
    "\n",
    "dS/dw = x\n",
    "\n",
    "\n",
    "##  Recursos Visuais\n",
    "\n",
    "F√≥rmulas:  \n",
    "\n",
    "üîó https://excalidraw.com/#room=2e2f0832aab9b8c40d36,Qn0tn30UH_lvd19ZgVcO3A\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
