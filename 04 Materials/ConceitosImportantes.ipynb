{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a72f19",
   "metadata": {},
   "source": [
    "# Baseado no Kahoot \n",
    "\n",
    "### **1.O que é um neurônio artificial?**  \n",
    "  \n",
    "R: *Equação que ativa a somátoria calculada com entradas e pesos*  \n",
    "\n",
    "###  **2. Uma função de ativação serve para?**  \n",
    "  \n",
    "R: *Introduzir não-linearidade no neurônio.*  \n",
    "\n",
    "<img src=\"img/ativacao.png\" width=\"400\" height=\"300\">  \n",
    "  \n",
    "### **3.Sobre** ***Forward Pass*** **,é quando um neurônio analisa a consequência que o seu erro causou e se melhora baseando-se nisso**  \n",
    "\n",
    "R: *Falso!*  \n",
    "*Essa afirmação é falsa porque ela mistura o papel do forward pass com o do **backpropagation**. No forward pass, a rede \"chuta\" uma resposta com os pesos que ela tem naquele instante, não tem correção de erro e nem aprendizado, é apenas uma previsão*  \n",
    "\n",
    "### **4. O que é Convolução?**  \n",
    "  \n",
    "R: *Operação que aplica pesos treináveis sobre a imagem para extrair características e padrões. O filtro (kernel) percorre a imagem, em cada posição ele faz contas com os pixels daquela região. Aí, o resultado vira um novo valor em uma nova imagem (mapa de características)*  \n",
    "  \n",
    "### **5. O que é Dropout?**\n",
    "  \n",
    "R: *É uma técnica usada durante o treinamento de redes neurais para evitar overfitting. Esse mecanismo desativa aleatoriamente neurônios durante o treinamento, ele força a rede a não \"decorar os dados de treino\"*  \n",
    "  \n",
    "### **6. O que é MaxPooling**  \n",
    "  \n",
    "R: *Reduz o tamanho da imagem sem perder as informações mais importantes. A imagem é dividida em pequenas regiões (default: 2x2), em cada região o maior valor é selecionado e esses valores formam uma nova imagem menor. Assim, ele mantém as características mais importantes.*  \n",
    "   \n",
    "<img src=\"img/maxpooling.png\" width=\"550\" height=\"200\">  \n",
    "  \n",
    "### **7. Explique como funciona o cálculo da Convolução** \n",
    "  \n",
    "<img src=\"img/calculoConv1.png\" width=\"300\" height=\"300\">\n",
    "<b style=\"font-size: 28px;\">=</b>\n",
    "<img src=\"img/calculoConv2.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "R:  \n",
    "$(8 * (-1)) + (2 * (-1)) + (8 * (-1)) +$  \n",
    "$(5 * (-1)) + (8 * 8) + (5 * (-1))  +$   \n",
    "$(8 * (-1)) + (5 * (-1)) + (8 * (-1)) = 49$  \n",
    "  \n",
    "### **8. Pensando sobre uma 128x128. Após aplicarmos o fluxo abaixo, quantos mapas de características teremos?**  \n",
    "\n",
    "```python\n",
    "Conv2d(4, (3x3))\n",
    "Conv2d(8, (3x3))\n",
    "Conv2d(16, (3x3))\n",
    "```\n",
    "\n",
    "R: *16.*  \n",
    "*Em uma camada Conv2D, o número de mapas de características (feature maps) gerados é igual ao número de filtros dessa camada.* \n",
    "  \n",
    "\n",
    "*Conv2D(4, 3×3)  → gera 4 mapas de características\n",
    "Conv2D(8, 3×3)  → gera 8 mapas de características\n",
    "Conv2D(16, 3×3) → gera 16 mapas de características*\n",
    "\n",
    "<img src=\"img/zebra.png\" width=\"380\" height=\"300\">  \n",
    "  \n",
    "### **9. É recomendado que após o último bloco (conv, maxpooling) os mapas de características sejam 4x4, para datasets com imagens com 128x128 ou menos. Em casos de imagens maiores varia com o nível de complexidade exigido.**  \n",
    "  \n",
    "R: *Verdadeiro, mas com ressalvas.*  \n",
    "*Quanto menor o mapa final, mais o modelo passo de \"onde está essa informação\" para \"o que é essa informação\". Porém, o tamanho final depende da complexidade do problema, quantidade de dados, profundidade da rede e necessidade de preserver informação especial.*  \n",
    "  \n",
    "### **10. Qual a consequência de ter um stride maior que a grade aplicada?**  \n",
    "  \n",
    "R: *Perda de informações e dados.*  \n",
    "  \n",
    "### **Sobre *Backpropagation*, é quando um neurônio analisa a consequência que seu erro causou e se melhora baseando-se nisso.**  \n",
    "  \n",
    "R: *Verdadeiro.*  \n",
    "*É o mecanismo que diz à rede como mudar cada peso para errar menos na próxima vex, ou seja, a rede neural aprende com o seus erros.*  \n",
    "  \n",
    "### **Para um modelo onde as labels são categóricos sem one-hot, que métrica devemos utilizar?** \n",
    "  \n",
    "R: `sparse_categorical_accuracy`.  \n",
    "*Labels inteiros (sem one-hot) → ``sparse_categorical_accuracy``\n",
    "\n",
    "Labels em one-hot → ``categorical_accuracy``*\n",
    "  \n",
    "### **Qual é qual?**  \n",
    "  \n",
    "<img src=\"img/fVSg.png\" width=\"600\" height=\"250\">  \n",
    "  \n",
    "R: `flatten()` e `globalaveragepooling2D()`  \n",
    "  \n",
    "### **O que são Batches?**  \n",
    "  \n",
    "R: *São pequenos grupos de dados usados para treinar a rede neural.  \n",
    "O dataset é dividido em vários lotes (batches), cada batch passa pela rede e após cada batch, os pesos são atualizados.*  \n",
    "  \n",
    "* **batch** -> um grupo de amostras  \n",
    "  **epoch** -> quando todos os batches do datase foram usados uma vez*  \n",
    "\n",
    "*Em resumo, batch é o tamanho do \"pacote\" de dados usado em cada atualização dos pesos da rede.*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
